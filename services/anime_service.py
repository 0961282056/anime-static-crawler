from typing import List, Dict, Tuple, Optional
from bs4 import BeautifulSoup
import requests, os, json, hashlib, time, re, logging
import multiprocessing
# FIX: ç¢ºä¿å°å…¥ cloudinary.api ä»¥ä¾›æª¢æŸ¥ç©ºé–“ä½¿ç”¨ç‹€æ³
import cloudinary, cloudinary.uploader, cloudinary.utils, cloudinary.api 
from dotenv import load_dotenv
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from datetime import datetime

# ã€å‡ç´š 1ã€‘å¼•å…¥ Pydantic æ¨¡å‹
from models import Anime
from config import Config

# ------------------------------------------------------
# åˆå§‹åŒ–èˆ‡è¨­å®š
# ------------------------------------------------------
load_dotenv()
logging.basicConfig(level=logging.INFO) 
logger = logging.getLogger(__name__)
logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)

CACHE_FILE = os.path.join(os.getcwd(), 'cloudinary_cache.json')
JSON_DIR = os.path.join(os.getcwd(), 'dist', 'data') # æ–°å¢ JSON_DIR å¸¸æ•¸

# ------------------------------------------------------
# requests Session & Pool è¨­å®š (ç•¥)
# ------------------------------------------------------
pool_size = 5
retry_strategy = Retry(
    total=3, backoff_factor=0.5,
    status_forcelist=[429, 500, 502, 503, 504]
)
adapter = HTTPAdapter(pool_connections=pool_size, pool_maxsize=pool_size,
                      max_retries=retry_strategy)

cloudinary_adapter = HTTPAdapter(pool_connections=4, pool_maxsize=4,
                                 max_retries=retry_strategy)

SEASON_TO_MONTH = Config.SEASON_TO_MONTH
WEEKDAY_MAP = Config.WEEKDAY_MAP

# ------------------------------------------------------
# é€²ç¨‹é–“å…±äº«æ•¸æ“š (ç•¥)
# ------------------------------------------------------
session_global = None 
cloudinary_config_global = {} 
manager_lock_global = None
manager_cache_global = None

def init_worker(shared_lock, shared_cache_dict):
    """æ¯å€‹é€²ç¨‹å•Ÿå‹•æ™‚åˆå§‹åŒ–è³‡æº"""
    global session_global, cloudinary_config_global, manager_lock_global, manager_cache_global
    
    manager_lock_global = shared_lock
    manager_cache_global = shared_cache_dict
    
    session_global = requests.Session()
    session_global.mount("http://", adapter)
    session_global.mount("https://", adapter)
    session_global.mount("https://api.cloudinary.com", cloudinary_adapter)
    
    cloudinary_config_global = {
        'cloud_name': os.getenv("CLOUDINARY_CLOUD_NAME"),
        'api_key': os.getenv("CLOUDINARY_API_KEY"),
        'api_secret': os.getenv("CLOUDINARY_API_SECRET"),
        'long_url_signature': True,
        'secure': True
    }
    cloudinary.config(**cloudinary_config_global)
    cloudinary.config(http_client=session_global)
    
    logging.basicConfig(level=logging.INFO)
    logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)

# ------------------------------------------------------
# ç°¡æ˜“å¿«å– (JSON) (ç•¥)
# ------------------------------------------------------
def load_local_cache() -> Dict:
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥å¿«å–å¤±æ•—: {e}")
            return {}
    return {}

def save_local_cache(data: Dict):
    try:
        filtered_data = {k: v for k, v in data.items() if k.startswith('cloudinary_')}
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(filtered_data, f, ensure_ascii=False, indent=4) 
    except Exception as e:
        logger.error(f"å„²å­˜å¿«å–å¤±æ•—: {e}")

# ------------------------------------------------------
# è¼”åŠ©å‡½å¼ (ç•¥)
# ------------------------------------------------------
def parse_date_time(anime: Dict) -> Tuple[int, float]:
    try:
        if anime["premiere_date"] == "ç„¡é¦–æ’­æ—¥æœŸ":
            return 8, float("inf")
        weekday = WEEKDAY_MAP.get(anime["premiere_date"], 7)
        if anime["premiere_time"] == "ç„¡é¦–æ’­æ™‚é–“":
            return weekday, 0.0
        match = re.match(r"(\d{1,2}):(\d{2})", anime["premiere_time"])
        if not match:
            raise ValueError
        hour, minute = int(match.group(1)), int(match.group(2))
        return weekday, hour + minute / 60.0
    except Exception:
        return 7, float("inf")

# ------------------------------------------------------
# Discord é€šçŸ¥ (ç•¥)
# ------------------------------------------------------
def send_discord_notification(status: str, year: str, season: str, count: int = 0, error_msg: str = ""):
    """ç™¼é€æ¼‚äº®çš„ Embed é€šçŸ¥åˆ° Discord"""
    webhook_url = os.getenv("DISCORD_WEBHOOK_URL")
    
    if not webhook_url: return

    color = 3066993 if status == "SUCCESS" else 15158332
    title = "âœ… å‹•ç•«çˆ¬èŸ²æ›´æ–°æˆåŠŸ" if status == "SUCCESS" else "ğŸš¨ å‹•ç•«çˆ¬èŸ²åŸ·è¡Œå¤±æ•—"
    
    description = f"**å­£åº¦**: {year} {season}\n"
    if status == "SUCCESS":
        description += f"**è³‡æ–™ç­†æ•¸**: {count} ç­†\n**ç‹€æ…‹**: å·²æ›´æ–°è‡³ GitHub & Cloudflare"
    else:
        description += f"**éŒ¯èª¤åŸå› **: {error_msg}\nè«‹æª¢æŸ¥ GitHub Actions Logsã€‚"

    payload = {
        "username": "Anime Crawler Bot",
        "avatar_url": "https://cdn-icons-png.flaticon.com/512/4712/4712109.png",
        "embeds": [{
            "title": title,
            "description": description,
            "color": color,
            "footer": {"text": f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"}
        }]
    }
    
    try:
        requests.post(webhook_url, json=payload, timeout=5)
    except Exception as e:
        logger.error(f"Discord é€šçŸ¥ç™¼é€å¤±æ•—: {e}")

# ------------------------------------------------------
# Cloudinary ç©ºé–“æª¢æŸ¥ (ä¿®å¾©ç‰ˆï¼šæ”¹æª¢æŸ¥ Credit é¡åº¦)
# ------------------------------------------------------
def check_cloudinary_storage_quota(usage_limit_percent: int = 90) -> bool:
    """æª¢æŸ¥ Cloudinary Credit é¡åº¦æ˜¯å¦è¶…éé™åˆ¶ç™¾åˆ†æ¯”ã€‚"""
    try:
        if not cloudinary.config().cloud_name:
            # ç¢ºä¿é…ç½®å·²è¼‰å…¥
            cloudinary.config(
                cloud_name=os.getenv("CLOUDINARY_CLOUD_NAME"),
                api_key=os.getenv("CLOUDINARY_API_KEY"),
                api_secret=os.getenv("CLOUDINARY_API_SECRET"),
                secure=True
            )
        
        usage_data = cloudinary.api.usage()
        
        # ğŸ¯ é—œéµä¿®å¾©ï¼šå¾ 'credits' æ¬„ä½ç²å–ä½¿ç”¨ç™¾åˆ†æ¯”
        credits_data = usage_data.get('credits', {})
        usage_percent = credits_data.get('used_percent')
        
        # å¦‚æœ 'used_percent' ä¸å­˜åœ¨ï¼Œå‰‡é€€å›èˆŠçš„æª¢æŸ¥æ–¹å¼ï¼ˆä»¥é˜²è¬ä¸€ï¼‰
        if usage_percent is None:
             logger.warning(f"Cloudinary API usage æ•¸æ“šä¸å®Œæ•´ï¼Œç„¡æ³•æª¢æŸ¥é¡åº¦ã€‚åŸå§‹å›æ‡‰: {usage_data}")
             return True # è·³éæª¢æŸ¥

        logger.info(f"Cloudinary é¡åº¦ä½¿ç”¨ç‡: {usage_percent:.2f}% (é™åˆ¶: {usage_limit_percent}%)")
        
        if usage_percent >= usage_limit_percent:
            logger.error(f"ğŸš¨ Cloudinary é¡åº¦ä½¿ç”¨ç‡å·²é” {usage_percent:.2f}%ï¼Œå·²è¶…éé™åˆ¶ ({usage_limit_percent}%)ã€‚")
            return False 
        
        return True
        
    except Exception as e:
        logger.error(f"ç„¡æ³•æª¢æŸ¥ Cloudinary ç©ºé–“ä½¿ç”¨ç‹€æ³: {e}")
        return True

# ------------------------------------------------------
# åœ–ç‰‡è™•ç† (å·²ä¿®æ­£ä¸Šå‚³ç•«è³ªå•é¡Œ)
# ------------------------------------------------------
def upload_to_cloudinary(image_url: str, anime_name: str) -> str:
    """è™•ç†åœ–ç‰‡ä¸Šå‚³æˆ–å¿«å–å‘½ä¸­"""
    if image_url == "ç„¡åœ–ç‰‡":
        return "ç„¡åœ–ç‰‡"
    
    session = session_global
    local_cache = manager_cache_global 
    
    try:
        response = session.get(image_url, timeout=6)
        response.raise_for_status()
        
        content_hash = hashlib.md5(response.content).hexdigest()
        public_id = f"anime_covers/{content_hash}"
        cloudinary_key = f"cloudinary_{content_hash}"
        
        # æª¢æŸ¥å¿«å–
        with manager_lock_global:
            if cloudinary_key in local_cache:
                return local_cache[cloudinary_key]
        
        # âœ… FIX: ç§»é™¤ width/height/crop é™åˆ¶ï¼Œè®“ Cloudinary ä¸Šä¿ç•™åŸåœ–ç•«è³ª
        upload_result = cloudinary.uploader.upload(
            response.content,
            public_id=public_id, overwrite=True, invalidate=True,
            transformation=[
                {"quality": "auto", "fetch_format": "auto"} # åƒ…å„ªåŒ–æ ¼å¼å’Œå“è³ªï¼Œä¸é™åˆ¶å°ºå¯¸
            ]
        )
            
        # 2. é—œéµä¿®æ”¹ï¼šç”¢ç”Ÿ URL æ™‚ï¼Œç§»é™¤å°ºå¯¸é™åˆ¶
        # ç”Ÿæˆçš„ URL ä¸åŒ…å« width=300, height=450, crop="limit"ï¼Œ
        # é€™æ¨£å¿«å–å’Œ JSON ä¸­å„²å­˜çš„å°±æ˜¯ Cloudinary ä¸Šçš„åŸåœ–é€£çµ
        url, _ = cloudinary.utils.cloudinary_url(
            upload_result["public_id"],
            fetch_format="auto", 
            quality="auto:best" # ä½¿ç”¨ auto:best ç¢ºä¿æœ€é«˜å“è³ª
        )
        
        with manager_lock_global:
            local_cache[cloudinary_key] = url
        
        logger.info(f"[UPLOAD] {anime_name} ä¸Šå‚³å®Œæˆ (åŸåœ–ä¿ç•™, WebPå„ªåŒ–)")
        return url

    except Exception as e:
        logger.error(f"[ERROR] {anime_name} åœ–ç‰‡è™•ç†å¤±æ•—: {e}")
        return image_url

def worker_process_anime(item_html_str: str) -> Optional[Dict]:
    """Worker: è§£æä¸¦ä½¿ç”¨ Pydantic é©—è­‰è³‡æ–™ (ç•¥)"""
    try:
        item = BeautifulSoup(item_html_str, "lxml").find("div", class_="CV-search")
        if not item: return None
        
        anime_name_elem = item.find("h3", {"class": "entity_localized_name"})
        anime_name = anime_name_elem.get_text(strip=True) if anime_name_elem else None
        
        premiere_date_elem = item.find("div", {"class": "time_today main_time"})
        premiere_date, premiere_time = None, None
        
        if premiere_date_elem:
            text = premiere_date_elem.get_text(strip=True)
            week_match = re.search(r"æ¯é€±([ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©])", text)
            if week_match: premiere_date = week_match.group(1)
            time_match = re.search(r"(\d{1,2})æ™‚(\d{1,2})åˆ†", text)
            if time_match: premiere_time = f"{int(time_match.group(1)):02d}:{int(time_match.group(2)):02d}"

        image_tag = item.find("div", {"class": "overflow-hidden anime_cover_image"})
        image_url = image_tag.img["src"] if image_tag and image_tag.img else "ç„¡åœ–ç‰‡"
        
        # åŸ·è¡Œåœ–ç‰‡ä¸Šå‚³
        anime_image_url = upload_to_cloudinary(image_url, anime_name or "æœªçŸ¥") 

        story_elem = item.find("div", {"class": "anime_story"})
        story = story_elem.get_text(strip=True) if story_elem else None
        
        # ã€å‡ç´š 1ã€‘ä½¿ç”¨ Pydantic æ¨¡å‹å»ºç«‹èˆ‡é©—è­‰
        anime_obj = Anime(
            bangumi_id=item.get("acgs-bangumi-data-id", "æœªçŸ¥ID"),
            anime_name=anime_name,
            anime_image_url=anime_image_url,
            premiere_date=premiere_date,  
            premiere_time=premiere_time,  
            story=story                   
        )
        
        # è½‰å› dict ä¾›å¾ŒçºŒè™•ç†
        return anime_obj.model_dump()

    except Exception as exc:
        logger.warning(f"è™•ç†å¤±æ•—: {exc}")
        return None

# ------------------------------------------------------
# æ–°å¢ï¼šæ™ºèƒ½æ¸…ç†è¼”åŠ©å‡½å¼
# ------------------------------------------------------
def find_oldest_season_file() -> Optional[Tuple[str, str]]:
    """æ‰¾å‡º dist/data ä¸‹æœ€èˆŠçš„ YYYY_SEASON.json æª”æ¡ˆ (Year, Season)"""
    if not os.path.isdir(JSON_DIR):
        return None

    # åªé¸å–ç¬¦åˆ YYYY_SEASON.json æ ¼å¼çš„æª”æ¡ˆ
    json_files = [f for f in os.listdir(JSON_DIR) if re.match(r'^\d{4}_(æ˜¥|å¤|ç§‹|å†¬)\.json$', f)]
    if not json_files:
        return None

    def get_sort_key(filename):
        # filename format: YYYY_SEASON.json
        match = re.match(r'^(\d{4})_(\S+)\.json$', filename)
        if not match:
            return float('inf')
        year = int(match.group(1))
        season_name = match.group(2)
        # ä½¿ç”¨ Config.SEASON_TO_MONTH é€²è¡Œæ’åºï¼Œç¢ºä¿å­£ç¯€é †åºæ­£ç¢º (å†¬, æ˜¥, å¤, ç§‹)
        month = Config.SEASON_TO_MONTH.get(season_name, 13) 
        return year * 100 + month

    oldest_file = min(json_files, key=get_sort_key)
    match = re.match(r'^(\d{4})_(\S+)\.json$', oldest_file)
    if match:
        return match.group(1), match.group(2)
    return None

def cleanup_specific_season(year: str, season: str, folder_prefix: str = "anime_covers/") -> int:
    """æ¸…ç†æŒ‡å®šå­£åº¦åœ¨ Cloudinary ä¸Šçš„åœ–ç‰‡å’Œæœ¬åœ° JSON"""
    json_filename = f'{year}_{season}.json'
    json_path = os.path.join(JSON_DIR, json_filename)
    public_ids_to_delete = []
    
    logger.warning(f"ğŸ§¹ é–‹å§‹æ¸…ç†æœ€èˆŠå­£åº¦: {year} {season} çš„åœ–ç‰‡è³‡æºã€‚")

    # 1. å¾ JSON å»ºç«‹å¾…åˆªé™¤ Public ID åˆ—è¡¨
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for anime in data.get('anime_list', []):
                    url = anime.get('anime_image_url')
                    if url and 'cloudinary.com' in url:
                        # å¾ URL ä¸­è§£æå‡º Public ID
                        match = re.search(r'v\d+/({})/([\w]+)'.format(folder_prefix.strip('/')), url)
                        if match:
                            public_id = f"{match.group(1)}/{match.group(2)}"
                            public_ids_to_delete.append(public_id)
        except Exception as e:
            logger.error(f"è®€å–æˆ–è§£æ JSON æª”æ¡ˆå¤±æ•—: {json_path}. éŒ¯èª¤: {e}")
    else:
        logger.warning(f"æ‰¾ä¸åˆ° {json_path}ï¼Œè·³éç™½åå–®å»ºç«‹ã€‚")

    deleted_count = 0
    if public_ids_to_delete:
        logger.info(f"æº–å‚™åˆªé™¤ {year} {season} ç›¸é—œçš„ {len(public_ids_to_delete)} ç­†é›²ç«¯è³‡æº...")
        
        # 2. åŸ·è¡Œæ‰¹é‡åˆªé™¤
        batch_size = 100
        for i in range(0, len(public_ids_to_delete), batch_size):
            batch = public_ids_to_delete[i:i + batch_size]
            try:
                delete_result = cloudinary.api.delete_resources(
                    batch, resource_type="image", type="upload"
                )
                deleted_count += len(delete_result.get('deleted', {}))
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆªé™¤æ™‚ç™¼ç”ŸéŒ¯èª¤ (æ‰¹æ¬¡ {i//batch_size + 1}): {e}")
                
        logger.info(f"âœ… Cloudinary æˆåŠŸåˆªé™¤ {year} {season} è³‡æºå…± {deleted_count} ç­†ã€‚")
        
    # 3. åˆªé™¤æœ¬åœ° JSON æª”æ¡ˆ
    if os.path.exists(json_path):
        try:
            os.remove(json_path)
            logger.info(f"âœ… æˆåŠŸåˆªé™¤æœ¬åœ° JSON æª”æ¡ˆ: {json_filename}")
        except Exception as e:
            logger.error(f"åˆªé™¤æœ¬åœ° JSON æª”æ¡ˆå¤±æ•—: {e}")

    # 4. åŒæ­¥æ¸…é™¤æœ¬åœ°å¿«å–
    if public_ids_to_delete:
        deleted_public_ids_for_cache = set(f"cloudinary_{pid.split('/')[-1]}" for pid in public_ids_to_delete)
        local_cache = load_local_cache()
        
        keys_to_delete = set(local_cache.keys()) & deleted_public_ids_for_cache
        
        if keys_to_delete:
            for key in keys_to_delete:
                del local_cache[key]
            save_local_cache(local_cache)
            logger.info(f"âœ… æˆåŠŸåŒæ­¥ç§»é™¤ {len(keys_to_delete)} ç­†æœ¬åœ°å¿«å–è¨˜éŒ„ã€‚")
            
    # âœ… æ–°å¢ï¼šç™¼é€æ¸…ç†æˆåŠŸçš„ Discord é€šçŸ¥
    if deleted_count > 0:
        send_discord_notification(
            status="FAILURE", 
            year=year, 
            season=season, 
            count=deleted_count, 
            error_msg=f"ğŸ§¹ è­¦å‘Šï¼šå·²å•Ÿå‹•æ™ºèƒ½æ¸…ç†ï¼ŒæˆåŠŸåˆªé™¤ {deleted_count} ç­† {year} {season} çš„åœ–ç‰‡è³‡æºã€‚ç©ºé–“å·²é‡‹æ”¾ã€‚"
        )

    return deleted_count

# ------------------------------------------------------
# ä¸»çˆ¬èŸ²é‚è¼¯ (æ–°å¢æ™ºèƒ½æ¸…ç†æ­¥é©Ÿ)
# ------------------------------------------------------
def fetch_anime_data(year: str, season: str, cache=None) -> List[Dict]:
    """ä¸»å‡½å¼ï¼šåŒ…å«ç©ºé–“æª¢æŸ¥èˆ‡æ™ºèƒ½æ¸…ç†æ©Ÿåˆ¶"""
    
    if season not in SEASON_TO_MONTH:
        return [{"error": "å­£ç¯€ç„¡æ•ˆ"}]

    url = f"https://acgsecrets.hk/bangumi/{year}{SEASON_TO_MONTH[season]:02d}/"
    
    with multiprocessing.Manager() as manager:
        try:
            # ğŸ¯ ç©ºé–“æª¢æŸ¥èˆ‡æ™ºèƒ½æ¸…ç†è¿´åœˆ
            max_cleanup_attempts = 3
            quota_check_ok = check_cloudinary_storage_quota()
            
            for attempt in range(max_cleanup_attempts):
                if quota_check_ok:
                    break
                    
                logger.warning(f"å˜—è©¦åŸ·è¡Œæ™ºèƒ½æ¸…ç† (ç¬¬ {attempt + 1} æ¬¡)...")
                oldest_season = find_oldest_season_file()
                
                if oldest_season is None:
                    logger.error("âŒ æ™ºèƒ½æ¸…ç†å¤±æ•—ï¼šæ‰¾ä¸åˆ°å¯åˆªé™¤çš„èˆŠå­£åº¦ JSON æª”æ¡ˆã€‚")
                    break
                    
                oldest_year, oldest_season_name = oldest_season
                # é¿å…åˆªé™¤æ­£åœ¨çˆ¬å–çš„å­£åº¦
                if (oldest_year, oldest_season_name) == (year, season):
                    logger.error("âŒ æ™ºèƒ½æ¸…ç†å¤±æ•—ï¼šæœ€èˆŠå­£åº¦ç‚ºç•¶å‰çˆ¬å–å­£åº¦ï¼Œç„¡æ³•åˆªé™¤ã€‚")
                    break

                # åŸ·è¡Œæ¸…ç†
                cleanup_specific_season(oldest_year, oldest_season_name)
                
                # é‡æ–°æª¢æŸ¥é…é¡
                quota_check_ok = check_cloudinary_storage_quota()
                
            if not quota_check_ok:
                logger.error("âŒ çˆ¬èŸ²åœæ­¢ï¼šCloudinary ç©ºé–“ä¸è¶³ï¼Œå¤šæ¬¡æ¸…ç†å¾Œä»ç„¡æ³•è§£æ±ºã€‚")
                send_discord_notification("FAILURE", year, season, 0, "Cloudinary ç©ºé–“ä¸è¶³ï¼Œæ™ºèƒ½æ¸…ç†å¤±æ•—ï¼Œå·²åœæ­¢çˆ¬èŸ²ã€‚")
                return [{"error": "Cloudinary ç©ºé–“ä¸è¶³ï¼Œæ™ºèƒ½æ¸…ç†å¤±æ•—"}]
            
            # ------------------------------------------------------
            # æ­£å¸¸çˆ¬èŸ²é‚è¼¯é–‹å§‹
            # ------------------------------------------------------
            
            # 1. æŠ“å– HTML 
            with requests.Session() as s:
                s.mount("http://", adapter)
                s.mount("https://", adapter)
                response = s.get(url, timeout=10) 
                response.raise_for_status()
            response.encoding = "utf-8"
            
            # 2. è§£æ
            soup = BeautifulSoup(response.text, "lxml")
            anime_items = soup.select("div#acgs-anime-list div.CV-search")
            if not anime_items:
                msg = f"{year} {season} ä¾†æºç¶²ç«™ç„¡è³‡æ–™ (HTMLçµæ§‹æ­£ç¢ºä½†ç„¡é …ç›®)"
                logger.warning(msg)
                return []

            item_html_strings = [str(item) for item in anime_items]
            
            # 3. åˆå§‹åŒ–å…±äº«è³‡æº
            shared_lock = manager.Lock()
            shared_cache_dict = manager.dict() 
            shared_cache_dict.update(load_local_cache())
            
            # 4. å¤šé€²ç¨‹è™•ç†
            max_workers = os.cpu_count() or 1
            with multiprocessing.Pool(processes=max_workers, initializer=init_worker, initargs=(shared_lock, shared_cache_dict)) as pool:
                results = pool.map(worker_process_anime, item_html_strings)

            anime_list = [res for res in results if res is not None]
            
            if not anime_list:
                error_msg = f"{year} {season} çˆ¬å–çµæœç‚ºç©º (å¯èƒ½è§£æå¤±æ•—)"
                logger.warning(f"âš ï¸ {error_msg}")
                send_discord_notification("FAILURE", year, season, 0, error_msg)
                return []

            sorted_list = sorted(anime_list, key=parse_date_time)
            
            # 5. å­˜å›å¿«å–
            save_local_cache(dict(shared_cache_dict))
            
            logger.info(f"æˆåŠŸçˆ¬å– {year} {season} å…± {len(sorted_list)} ç­†è³‡æ–™")
            
            # ã€å‡ç´š 3ã€‘ç™¼é€æˆåŠŸé€šçŸ¥
            send_discord_notification("SUCCESS", year, season, len(sorted_list))
            
            return sorted_list

        except Exception as e:
            logger.error(f"çˆ¬å–å¤±æ•—: {e}")
            send_discord_notification("FAILURE", year, season, 0, str(e))
            return [{"error": f"ç³»çµ±éŒ¯èª¤: {str(e)}"}]

def get_current_season(month: int) -> str:
    if 1 <= month <= 3: return "å†¬"
    if 4 <= month <= 6: return "æ˜¥"
    if 7 <= month <= 9: return "å¤"
    return "ç§‹"