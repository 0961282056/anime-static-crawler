import os
import logging
import json
import re  # ç¢ºä¿å°å…¥ï¼Œç”¨æ–¼è§£æ Cloudinary URL
from datetime import datetime, timedelta
import cloudinary.api
from dotenv import load_dotenv
from config import Config  # å°å…¥ Config ä»¥ä½¿ç”¨å­£åº¦æœˆä»½å°æ‡‰

# ------------------------------------------------------
# åˆå§‹åŒ–èˆ‡è¨­å®š
# ------------------------------------------------------
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è¨­å®šå¿«å–æª”æ¡ˆè·¯å¾‘å’Œè³‡æ–™ç›®éŒ„
CACHE_FILE = os.path.join(os.getcwd(), 'cloudinary_cache.json')
JSON_DIR = os.path.join(os.getcwd(), 'dist', 'data')

# è¨­ç½® Cloudinary é€£ç·š
try:
    cloudinary.config(
        cloud_name=os.getenv("CLOUDINARY_CLOUD_NAME"),
        api_key=os.getenv("CLOUDINARY_API_KEY"),
        api_secret=os.getenv("CLOUDINARY_API_SECRET"),
        secure=True
    )
except Exception as e:
    logger.error(f"Cloudinary é…ç½®å¤±æ•—: {e}. è«‹æª¢æŸ¥ CLOUDINARY_* è®Šæ•¸ã€‚")
    pass 

def load_local_cache() -> dict:
    """è¼‰å…¥æœ¬åœ°çš„ Cloudinary å¿«å–æª”æ¡ˆã€‚"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                content = json.load(f)
                # ç¢ºä¿å›å‚³çš„æ˜¯å­—å…¸ï¼Œå¦‚æœæª”æ¡ˆæ˜¯ç©ºçš„æˆ–æ˜¯ç©ºåˆ—è¡¨ï¼Œå›å‚³ç©ºå­—å…¸
                return content if isinstance(content, dict) else {}
        except json.JSONDecodeError:
            logger.warning(f"å¿«å–æª”æ¡ˆ {CACHE_FILE} æ ¼å¼éŒ¯èª¤æˆ–ç‚ºç©ºï¼Œè¦–ç‚ºç©ºå¿«å–ã€‚")
            return {}
    return {}

def save_local_cache(cache_data: dict):
    """å„²å­˜æœ¬åœ°çš„ Cloudinary å¿«å–æª”æ¡ˆã€‚"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logger.error(f"ç„¡æ³•å„²å­˜å¿«å–æª”æ¡ˆ: {CACHE_FILE}ã€‚éŒ¯èª¤: {e}")

def get_quarters_to_keep(years_to_keep: int) -> set:
    """è¨ˆç®—å‡ºæœ€è¿‘ n å¹´å…§ (åŒ…å«ç•¶å‰) éœ€è¦ä¿ç•™çš„å­£åº¦ (Year, Season) é›†åˆã€‚"""
    now = datetime.now()
    quarters_to_keep = set()
    
    # éæ­·å¾ n å¹´å‰åˆ°ç¾åœ¨çš„æ‰€æœ‰æœˆä»½
    start_date = now - timedelta(days=365 * years_to_keep)
    
    current_year = start_date.year
    current_month = start_date.month

    while current_year <= now.year or (current_year == now.year and current_month <= now.month):
        # ç¢ºä¿ season_month æ˜¯ 1, 4, 7, 10
        season_month = (current_month - 1) // 3 * 3 + 1
        
        # æ ¹æ“šæœˆä»½è¨ˆç®—å­£åº¦åç¨±
        season_name = next((k for k, v in Config.SEASON_TO_MONTH.items() if v == season_month), 'æœªçŸ¥')
        
        if season_name != 'æœªçŸ¥':
            quarters_to_keep.add((str(current_year), season_name))

        # é€²åˆ°ä¸‹ä¸€å€‹å­£åº¦
        current_month += 3
        if current_month > 12:
            current_month -= 12
            current_year += 1
            
    # å°‡æœªä¾†ä¸€å­£ä¹Ÿç´å…¥ä¿ç•™ç¯„åœ
    current_quarter_start_month = (now.month - 1) // 3 * 3 + 1
    next_month = current_quarter_start_month + 3
        
    if next_month > 12:
        next_year = now.year + 1
        next_month -= 12
    else:
        next_year = now.year
        
    next_season_name = next((k for k, v in Config.SEASON_TO_MONTH.items() if v == next_month), None)

    if next_season_name:
         quarters_to_keep.add((str(next_year), next_season_name))

    return quarters_to_keep


def cleanup_cloudinary_resources(years_to_keep: int = 15, folder_prefix: str = "anime_covers/") -> int:
    """
    1. æª¢æŸ¥æœ¬åœ°å¿«å–ï¼Œè‹¥ç‚ºç©ºå‰‡é€²å…¥ã€Œå…¨é¢é‡ç½®æ¨¡å¼ã€ã€‚
    2. è‹¥ä¸ç‚ºç©ºï¼Œå‰‡å»ºç«‹ç™½åå–®ï¼Œåƒ…ä¿ç•™æœ€è¿‘ n å¹´çš„åœ–ç‰‡ã€‚
    3. éæ­· Cloudinary åˆªé™¤ä¸éœ€è¦çš„åœ–ç‰‡ã€‚
    """
    
    if not os.getenv("CLOUDINARY_API_KEY"):
        logger.warning("Cloudinary API æ†‘è­‰ç¼ºå¤±ï¼Œè·³éåœ–ç‰‡æ¸…ç†èˆ‡å¿«å–åŒæ­¥ã€‚")
        return 0

    # ------------------------------------------------------
    # æ­¥é©Ÿ 0: æª¢æŸ¥å¿«å–ç‹€æ…‹ (æ±ºå®šæ˜¯å¦å…¨æ•¸åˆªé™¤)
    # ------------------------------------------------------
    local_cache = load_local_cache()
    public_ids_to_keep = set()
    
    # å¦‚æœå¿«å–ç‚ºç©ºï¼Œè§¸ç™¼å…¨é¢æ¸…ç†
    if not local_cache:
        logger.warning(f"âš ï¸ æª¢æ¸¬åˆ°æœ¬åœ°å¿«å– ({CACHE_FILE}) ç‚ºç©ºæˆ–ä¸å­˜åœ¨ã€‚")
        logger.warning(f"ğŸš¨ å°‡åŸ·è¡Œã€Œå…¨é¢é‡ç½®ã€æ¨¡å¼ï¼šåˆªé™¤ Cloudinary ä¸Šæ‰€æœ‰ '{folder_prefix}' ä¸‹çš„è³‡æºï¼")
        # public_ids_to_keep ä¿æŒç‚ºç©º set()ï¼Œé€™æœƒå°è‡´å¾ŒçºŒæ­¥é©Ÿåˆªé™¤æ‰€æœ‰æ‰¾åˆ°çš„è³‡æº
        
    else:
        # ------------------------------------------------------
        # æ­¥é©Ÿ 1: å»ºç«‹ Public ID ç™½åå–® (æ­£å¸¸æ¨¡å¼)
        # ------------------------------------------------------
        logger.info(f"--- æ­¥é©Ÿ 1: å»ºç«‹ Cloudinary åœ–ç‰‡ç™½åå–® (ä¿ç•™æœ€è¿‘ {years_to_keep} å¹´çš„å­£åº¦è³‡æ–™) ---")
        
        quarters_to_keep = get_quarters_to_keep(years_to_keep)
        
        for year, season in quarters_to_keep:
            json_filename = f'{year}_{season}.json'
            json_path = os.path.join(JSON_DIR, json_filename)
            
            if os.path.exists(json_path):
                try:
                    with open(json_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        for anime in data.get('anime_list', []):
                            url = anime.get('anime_image_url')
                            if url and 'cloudinary.com' in url:
                                # å¾ Cloudinary URL ä¸­è§£æå‡º Public ID
                                match = re.search(r'v\d+/({})/([\w]+)'.format(folder_prefix.strip('/')), url)
                                if match:
                                    public_id = f"{match.group(1)}/{match.group(2)}"
                                    public_ids_to_keep.add(public_id)
                                
                except Exception as e:
                    logger.error(f"è®€å–æˆ–è§£æ JSON æª”æ¡ˆå¤±æ•—: {json_path}. éŒ¯èª¤: {e}")

        logger.info(f"ç™½åå–®å»ºç«‹å®Œæˆã€‚å…± {len(public_ids_to_keep)} ç­†åœ–ç‰‡ (ä¾†è‡ª {len(quarters_to_keep)} å€‹å­£åº¦) å°‡è¢«ä¿ç•™ã€‚")


    # ------------------------------------------------------
    # æ­¥é©Ÿ 2: éæ­· Cloudinary è³‡æºä¸¦åˆªé™¤ä¸åœ¨ç™½åå–®çš„
    # ------------------------------------------------------
    total_deleted_cloud = 0
    
    logger.info("--- æ­¥é©Ÿ 2: éæ­·é›²ç«¯è³‡æºä¸¦åˆªé™¤ç™½åå–®ä»¥å¤–çš„åœ–ç‰‡ ---")

    public_ids_to_delete_all = [] 
    next_cursor = None
    
    while True:
        try:
            # ä½¿ç”¨ resources() éæ­·æ‰€æœ‰è³‡æº
            resources_result = cloudinary.api.resources(
                type='upload', 
                prefix=folder_prefix, 
                max_results=500,
                next_cursor=next_cursor,
            )
            
            resources_list = resources_result.get('resources', [])
            
            if not resources_list:
                break

            current_batch_to_delete = []
            for res in resources_list:
                public_id = res.get('public_id')
                
                # å¦‚æœ Public ID ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­ (æˆ–æ˜¯ç™½åå–®ç‚ºç©º)ï¼Œå‰‡æ¨™è¨˜åˆªé™¤
                if public_id and public_id not in public_ids_to_keep:
                    current_batch_to_delete.append(public_id)

            if current_batch_to_delete:
                public_ids_to_delete_all.extend(current_batch_to_delete)
                logger.info(f"ç™¼ç¾ {len(current_batch_to_delete)} ç­†å¾…åˆªé™¤è³‡æº...")

            next_cursor = resources_result.get('next_cursor')
            if not next_cursor:
                break
            
        except Exception as e:
            logger.error(f"Cloudinary éæ­·éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            break
            
    # åŸ·è¡Œæ‰¹é‡åˆªé™¤
    if public_ids_to_delete_all:
        logger.info(f"--- æ­¥é©Ÿ 2B: æº–å‚™åˆªé™¤ç¸½å…± {len(public_ids_to_delete_all)} ç­†é›²ç«¯è³‡æº ---")
        
        batch_size = 100
        for i in range(0, len(public_ids_to_delete_all), batch_size):
            batch = public_ids_to_delete_all[i:i + batch_size]
            try:
                # æ‰¹é‡åˆªé™¤
                delete_result = cloudinary.api.delete_resources(
                    batch, 
                    resource_type="image", 
                    type="upload"
                )
                
                deleted_count = len(delete_result.get('deleted', {})) 
                
                total_deleted_cloud += deleted_count
                logger.info(f"æˆåŠŸåˆªé™¤ä¸€æ‰¹ {deleted_count} ç­†é›²ç«¯è³‡æºã€‚")
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆªé™¤æ™‚ç™¼ç”ŸéŒ¯èª¤ (æ‰¹æ¬¡ {i//batch_size + 1}): {e}")
                
    logger.info(f"--- é›²ç«¯æ¸…ç†å®Œæˆã€‚ç¸½å…±åˆªé™¤ {total_deleted_cloud} ç­†é›²ç«¯è³‡æºã€‚---")

    # ------------------------------------------------------
    # æ­¥é©Ÿ 3: åŒæ­¥æ¸…ç†æœ¬åœ°å¿«å– (åˆªé™¤å·²åˆªé™¤ Public ID çš„å¿«å–è¨˜éŒ„)
    # ------------------------------------------------------
    
    # å¦‚æœæ˜¯å…¨é¢é‡ç½®æ¨¡å¼ (local_cache ç‚ºç©º)ï¼Œé€™ä¸€æ­¥å…¶å¯¦æ²’ä»€éº¼å¥½åˆªçš„ï¼Œä½†é‚è¼¯é€šç”¨
    if public_ids_to_delete_all:
        deleted_public_ids_for_cache = set(f"cloudinary_{pid.split('/')[-1]}" for pid in public_ids_to_delete_all)
        total_deleted_cache = 0
        
        # é‡æ–°è®€å–ä¸€æ¬¡å¿«å– (é˜²æ­¢åœ¨åŸ·è¡Œéç¨‹ä¸­å¿«å–è¢«å…¶ä»–é€²ç¨‹ä¿®æ”¹ï¼Œé›–ç„¶æ­¤è…³æœ¬é€šå¸¸å–®ç¨åŸ·è¡Œ)
        local_cache = load_local_cache()
        original_cache_size = len(local_cache)
        
        if local_cache:
            # æ‰¾å‡ºå¿«å–ä¸­éœ€è¦åˆªé™¤çš„éµ
            keys_to_delete = set(local_cache.keys()) & deleted_public_ids_for_cache
            
            for key in keys_to_delete:
                del local_cache[key]
                total_deleted_cache += 1
                
            if total_deleted_cache > 0:
                save_local_cache(local_cache)
                logger.info(f"æˆåŠŸåŒæ­¥å¿«å–ã€‚å¾ {original_cache_size} ç­†è¨˜éŒ„ä¸­ç§»é™¤ {total_deleted_cache} ç­†å·²åˆªé™¤çš„åœ–ç‰‡å¿«å–ã€‚")
            else:
                logger.info("æœ¬åœ°å¿«å–ä¸­æ²’æœ‰æ‰¾åˆ°éœ€è¦ç§»é™¤çš„èˆŠè¨˜éŒ„ã€‚")
            
    logger.info(f"--- Cloudinary åœ–ç‰‡æ¸…ç†èˆ‡å¿«å–åŒæ­¥ä½œæ¥­å…¨éƒ¨å®Œæˆã€‚---")
    return total_deleted_cloud

if __name__ == '__main__':
    # åŸ·è¡Œæ¸…ç†ï¼Œé è¨­ä¿ç•™ 15 å¹´
    cleanup_cloudinary_resources(years_to_keep=15)