name: Scheduled Anime Crawler

on:
  schedule:
    # æ¯å¤© UTC 17:00 åŸ·è¡Œ (å°ç£æ™‚é–“ 01:00)
    - cron: '0 17 * * *'
  workflow_dispatch:

jobs:
  crawl-and-update:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: â¬‡ï¸ Checkout Repository
        uses: actions/checkout@v4

      # ã€å„ªåŒ– 1ã€‘é–‹å•Ÿ pip å¿«å–ï¼Œå¤§å¹…åŠ å¿«å®‰è£é€Ÿåº¦
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # è‡ªå‹•å¿«å– pip ä¾è³´

      # é€™è£¡æœƒå„ªå…ˆå¾å¿«å–é‚„åŸï¼Œè‹¥ requirements.txt æ²’è®Šï¼Œå¹¾ç§’é˜å°±è·‘å®Œ
      - name: âš™ï¸ Install dependencies
        run: |
          pip install -r requirements.txt

      # ã€å„ªåŒ– 2ã€‘æ˜ç¢ºæŒ‡å®š BUILD_ONLY ç‚º falseï¼Œç¢ºä¿åŸ·è¡Œçˆ¬èŸ²
      - name: ğŸ•·ï¸ Run Crawler
        env:
          CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
          CLOUDINARY_API_KEY: ${{ secrets.CLOUDINARY_API_KEY }}
          CLOUDINARY_API_SECRET: ${{ secrets.CLOUDINARY_API_SECRET }}
          BUILD_ONLY: false 
        run: |
          python generate_static.py

      - name: ğŸ’¾ Commit and Push new data
        run: |
          git config --global user.name 'GitHub Action Crawler'
          git config --global user.email 'action@github.com'
          
          # åŠ å…¥ dist è³‡æ–™å¤¾èˆ‡å¿«å–æª”
          git add dist/data/*.json
          git add cloudinary_cache.json
          
          # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´ï¼Œæœ‰è®Šæ›´æ‰ Commit
          if git diff --staged --quiet; then
            echo "No changes to commit."
          else
            git commit -m "ğŸ¤– Auto-update anime data & cache"
            git push
            echo "âœ… Data pushed to GitHub. Cloudflare Pages should trigger build now."
          fi
